#!/usr/bin/python
#
# LoadBalancer -- A server for distributing upload and download jobs
# between GeneTorrent server instances
#
################################################################################
# 
# Copyright 2011 Annai Systems, Inc.   All rights reserved.
# 
# Redistribution and use in source and binary forms, with or without modification, 
# are permitted provided that the following conditions are met:
# 
#    1. Redistributions of source code must retain the above copyright notice, 
#       this list of conditions and the following disclaimer.
# 
#    2. Redistributions in binary form must reproduce the above copyright notice, 
#       this list of conditions and the following disclaimer in the documentation 
#       and/or other materials provided with the distribution.
# 
# THIS SOFTWARE IS PROVIDED BY ANNAI SYSTEMS ''AS IS'' AND ANY EXPRESS
# OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
# WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE
# ARE DISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> OR CONTRIBUTORS
# BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY,
# OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT
# OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR
# BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF
# LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING
# NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS
# SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
# 
# The views and conclusions contained in the software and documentation are those 
# of the authors and should not be interpreted as representing official policies, 
# either expressed or implied, of Annai Systems, Inc.
#
# Created under contract by Cardinal Peak, LLC.   www.cardinalpeak.com
#
################################################################################

# Current version
revlevel = '@PACKAGE_VERSION@'

# Load a couple of useful modules
from optparse import OptionParser
from time import sleep
import os
import sys
import ConfigParser
import re
import shutil
import subprocess
import time
import signal
import syslog
import traceback
import pwd
import grp

# log()
#
# Emits a log message, given the current logging level
#
# Inputs:
# - level -- the level of this log message, from LogLevel above
# - msg -- the message to be logged
#
def log(level, msg):
    syslog.syslog(level, msg)


# compute_instance_queue_size()
#
# Returns the current size of the given instance queue.  
#
# Inputs:
# - q_dir -- the instance queue directory
#
# Outputs:
# - the size of the q_dir
#
# Note: at the moment, this function returns the sum of the sizes of
# the GTO files in q_dir.  This is different from (and not as accurate
# as) the total size of the files pointed to by the GTOs.  It's less
# computationally intensive to get the size of the GTO files, and as
# long as piece size is reasonably constant across GTOs, then the
# sizes of the GTOs is a relatively good proxy for the size of the
# files they contain.
#
def compute_instance_q_size(q_dir):
    size=0
    gtos = os.listdir(q_dir)
    for gto in gtos:
        if (re.match(".*\.(gto|GTO)$", gto) != None):
            size += os.path.getsize(os.path.join(q_dir, gto))
    return size


# build_worklist()
#
# Based on all the instance work-queues, builds a worklist ordered by
# least- to most-loaded.  The list is stored in the config, and is
# structured as a set of tuples:
#    [ (q_dir_1, size_1), (q_dir_2, size_2), ... ]
#
# ...where the q_dir's are the instance queues from the config, and
# the sizes represent the aggregate size of all the GTOs currently in
# that particular queue directory
#
# Inputs:
# - config -- the config dict, originally read from the config file
#
# Outputs:
# - the "worklist" element in config is created or updated
#
def build_worklist(config):
    log(syslog.LOG_INFO, "Rebuilding sorted list of instance queues")
    worklist=list()
    for q_dir in config['instance_qs']:
        size = compute_instance_q_size(q_dir)
        worklist.append( (q_dir, size) )
    worklist.sort(key=lambda item: item[1])
    config['worklist'] = worklist
    config['worklist_refresh'] = time.time() + config['refresh_interval']
    dump_worklist(config)


# get_expiration()
#
# Returns the expiration of the supplied GTO file, in human-readable
# form.  Useful for log messages.
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - gto -- the fully-qualified pathname to the GTO file
#
# Outputs:
# - ret -- a string which represents the expiration of the GTO
def get_expiration(config, gto):
    cmd = [config['gtoinfo'], '--get-key=expires on', gto]
    ret = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0]
    return ret.strip()


# get_infohash()
#
# Returns the infohash of the supplied GTO file.
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - gto -- the fully-qualified pathname to the GTO file
#
# Outputs:
# - ret -- a string which represents the infohash of the GTO
def get_infohash(config, gto):
    cmd = [config['gtoinfo'], '--infohash', gto]
    ret = subprocess.Popen(cmd, stdout=subprocess.PIPE).communicate()[0]
    return ret.strip()


# get_file_list()
#
# Returns the list of files from the supplied GTO file
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - gto -- the fully-qualified pathname to the GTO file
#
# Outputs: 
# - ret -- the files in the gto, in the form of a list of dicts.  Each
#     list element will be a dict with the elements 'filename' and
#     'length'
def get_file_list(config, gto):
    filelist = list()
    cmd = [config['gtoinfo'], '--list-files', gto]
    data_stream = subprocess.Popen(cmd, stdout=subprocess.PIPE)
    for line in data_stream.stdout:
        m = re.match("^(.+) ([0-9]+)$", line)
        if (not m):
            raise SyntaxError('Invalid file output: %s' % line)
        filelist.append({'filename' : m.group(1), 'length' : int(m.group(2))})
    return filelist


# set_download_flag()
#
# Modifies the supplied GTO file, adding the 'gt_download_mode' flag
#
# Inputs:
# - in_q -- the input queue directory that contains the GTO file
# - gto -- the gto file's basename.  Note the file can be found
#          at in_q/gto_file
# - config -- the config dict, originally read from the config file
#
# Outputs:
# the gto file is modified in-place
#
# TODO: the functions set_download_flag and add_expiration should be
# factored somehow
def set_download_flag(in_q, gto, config):
    gtofile = os.path.join(in_q, gto)
    cmd = [config['gtoinfo'], '--set-key', 'gt_download_mode', 'true', '--no-backup', gtofile]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret =  proc.wait()
    if (ret == 0):
        return 
    log(syslog.LOG_ALERT, "Error processing GTO file %s" % gtofile)
    log(syslog.LOG_ALERT, "  %s" % proc.communicate()[0])
    raise IOError("Could not modify %s" % gto)


# add_expiration()
#
# Adds or updates the expiration date in the GTO
#
# Inputs:
# - in_q -- the input queue directory that contains the GTO file
# - gto -- the gto file's basename.  Note the file can be found
#          at in_q/gto_file
# - config -- the config dict, originally read from the config file
#
# Outputs:
# the GTO file is modified in-place
#
def add_expiration(in_q, gto, config):
    gtofile = os.path.join(in_q, gto)
    cmd = [config['gtoinfo'], '--expires=%d' % config['expiration'], 
           '--no-backup', gtofile]
    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
    ret =  proc.wait()
    if (ret == 0):
        return 
    log(syslog.LOG_ALERT, "Error processing GTO file %s" % gtofile)
    log(syslog.LOG_ALERT, "  %s" % proc.communicate()[0])
    raise IOError("Could not modify %s" % gto)


# assign_gto()
#
# Does the work of processing a single input GTO file.
#
# Inputs:
# - in_q -- the input queue directory that contains the GTO file
# - gto -- the gto file's basename.  Note the file can be found
#          at in_q/gto_file
# - factor -- the number of work queues to assign this GTO file to
# - config -- the config dict, originally read from the config file
#
# On output, the GTO file has been processed: it has an expiration
# date assigned, and it is copied to one or more instance queues
#
def assign_gto(in_q, gto, factor, config):
    # get the full path and size of this GTO
    src_gto = os.path.join(in_q, gto)
    size = os.path.getsize(src_gto)

    # for each destination queue:
    #   - copy the GTO into place
    #   - update the worklist with the new size
    for i in range(factor):
        (dest_q, dest_q_size) = config['worklist'][i]
        dest_gto = os.path.join(dest_q, gto)
        try:
            if (os.path.exists(dest_gto)):
                # Hmph. We're about to overwrite a GTO file. Print a warning.
                old_exp = get_expiration(config, dest_gto)
                new_exp = get_expiration(config, src_gto)
                log(syslog.LOG_WARNING, "Warning: overwriting GTO %s (expires %s) with"
                    " GTO that expires on %s" 
                    % (pretty_pathname(config, dest_gto), old_exp, new_exp))
                os.remove(dest_gto)

            shutil.copy(src_gto, dest_gto)
            dest_q_size += size
            config['worklist'][i] = (dest_q, dest_q_size)
            log(syslog.LOG_INFO, "Assigned GTO %s to queue %s" 
                % (gto, pretty_pathname(config, dest_q)))
        except:
            log(syslog.LOG_ALERT, "Error assigning GTO %s to queue %s: %s"
                % (gto, pretty_pathname(config, dest_q), sys.exc_info()[1]))

    # finally, re-sort the worklist, and delete the src_gto
    config['worklist'].sort(key=lambda item: item[1])
    os.remove(src_gto)


# dump_worklist()
#
# Dumps the worklist in a human-readable format to syslog
#
# Inputs:
# - config -- the config dict, originally read from the config file
def dump_worklist(config):
    log(syslog.LOG_DEBUG, "Dumping current worklist")
    for (instance_q, instance_q_size) in config['worklist']:
        log(syslog.LOG_DEBUG, "  %s: %d" 
            % (pretty_pathname(config, instance_q), instance_q_size))


# make_writable()
#
# If we are currently able to both read and write the specified file,
# take no action and return.  IF we are currently able to read but not
# write the specified file, attempt to fix this problem by (a) making
# a copy of the file, (b) removing the original, and (c) moving the
# copy to the name of the original.
#
# Inputs:
# - path -- the path to the file
def make_writable(path):
    if os.access(path, os.W_OK):
        return True
    if not os.access(path, os.R_OK):
        raise IOError("Cannot read file: %s" % path)
    tmpfile="%s-copy" % path
    shutil.copy(path, tmpfile)
    os.rename(tmpfile, path)


# validate_gto()
#
# For download mode, this function performs an efficient heuristic
# intended to catch some forms of possible corruption within CGHub.
# For a given GTO, the function verifies that all files listed in the
# GTO are present, and that they have the expected sizes.  Note that
# this function does NOT run SHA-1 sums on the files, because that
# would be expensive.
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - gto -- the fully-qualified pathname to the GTO file
#
# Output: None.  If an error is encountered, error info is logged to
# syslog and an exception is raised
#
def validate_gto(config, gto):
    if not config['validate_download_gtos']:
        return
    filelist = get_file_list(config, gto)
    for f in filelist:
        path = os.path.join(config['save_root_dir'], f['filename'])
        if not os.path.isfile(path):
            raise IOError("Detected potential corruption: missing file %s" % path)
        if (os.path.getsize(path) != f['length']):
            raise IOError("Detected potential corruption on file %s: GTO expects length %d, "\
                              "but filesystem length is %d" 
                          % (path, f['length'], os.path.getsize(path)))
        

# save_gto()
#
# Save the specified GTO to the directory specified in the config file
# in the SaveUploadGTOs section
#
# If the destination file exists, then it must exactly match the
# source file, in which case the function returns.  If the destination
# file exists but does not match, an error is raised.
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - src_dir -- the source directory.  The full source file is src_dir/gto
# - gto -- the gto file
#
# Output:
# The GTO file is saved to the specified directory
#
def save_gto(config, src_dir, gto):
    dst_dir=os.path.join(config['save_root_dir'], os.path.splitext(gto)[0])
    if (not os.path.isdir(dst_dir)):
        os.mkdir(dst_dir, config['save_uuid_dir_perms'])
    os.chown(dst_dir, config['save_uuid_dir_owner'], config['save_uuid_dir_group'])

    src=os.path.join(src_dir, gto)
    dst=os.path.join(dst_dir, gto)

    if (os.path.isfile(dst)):
        # dest file exists; perform comparison
        src_hash=get_infohash(config, src)
        dst_hash=get_infohash(config, dst)
        if (dst_hash != src_hash):
            raise ValueError("File %s exists but does not match new GTO" % dst)

    shutil.copyfile(src, dst)

    os.chmod(dst, config['save_gto_perms'])
    os.chown(dst, config['save_gto_owner'], config['save_gto_group'])


# remove_previous_copies_of_gto()
#
# Iterate through all the instance queues, and remove any files that
# match the input GTO.
#
# Inputs:
# - gto -- the gto file
# - config -- the config dict, originally read from the config file
#
# Output: Any previous copies of the gto that exist in any instance
# queues will be deleted.
#
def remove_previous_copies_of_gto(gto, config):
    for q_dir in config['instance_qs']:
        f=os.path.join(q_dir, gto)
        if (os.path.exists(f)):
            os.remove(f)
            log(syslog.LOG_INFO, "Removed previous version of gto %s from instance queue %s"
                % (gto, pretty_pathname(config, q_dir)))

# process_input_q()
#
# Processes an input queue -- either upload or download
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - q_type -- one of the strings 'upload' or 'download'
#
# Output:
# - assigned: True if at least one file was processed, False otherwise
#
def process_input_q(config, q_type):
    q_dir = config["%s_q" % q_type]
    factor = config["%s_factor" % q_type]
    if (q_dir == "DISABLED"):
        return False
    gtos = os.listdir(q_dir)
    assigned = False
    for gto in gtos:
        log(syslog.LOG_NOTICE, "New GTO %s added to %s queue." % (gto, q_type))
        fullname=os.path.join(q_dir, gto)
        try:
            if (q_type == 'upload' and config['save_upload_gtos']):
                save_gto(config, q_dir, gto)
            make_writable(fullname)
            if (q_type == 'download'):
                set_download_flag(q_dir, gto, config)
                validate_gto(config, fullname)
            elif (q_type == 'upload'):
                remove_previous_copies_of_gto(gto, config)
            add_expiration(q_dir, gto, config)
            assign_gto(q_dir, gto, factor, config)
            assigned = True
        except:
            log(syslog.LOG_ALERT, "Error on GTO %s: %s" % (gto, sys.exc_info()[1]))
            try:
                os.remove(fullname)
                log(syslog.LOG_ALERT, "Removed invalid file %s" % fullname)
            except OSError:
                log(syslog.LOG_ALERT, "Could not remove invalid file %s" % fullname)
    return assigned


# mainloop()
#
# Implements the main loop.  Note this function never returns, unless
# a fatal error occurs or the user terminates operations with SIGINT
#
# Inputs:
# - config -- the config dict, originally read from the config file
#
def mainloop(config):
    log(syslog.LOG_NOTICE, "GTLoadBalancer started, entering main loop")

    while True:
        # (re)build the worklist
        if ('worklist' not in config or config['worklist_refresh'] < time.time()):
            build_worklist(config)

        # process the two input queues
        assigned = False

        if process_input_q(config, 'upload'):
            assigned = True

        if process_input_q(config, 'download'):
            assigned = True

        if assigned:
            dump_worklist(config)

        # poll to see if we should re-read our config file
        mtime = os.path.getmtime(config['config_file'])
        if (mtime > config['config_file_mtime']):
            try:
                newconfig = parse_config_file(config['config_file'])
                config = newconfig
                log(syslog.LOG_NOTICE, "Re-read configuration file successfully")
            except Exception as e:
                log(syslog.LOG_ALERT, "Error reading config file: %s" % e)
                log(syslog.LOG_ALERT, 
                    "Because of config file errors, proceeding with previous configuration")
                # still update the mtime; otherwise we'll get this
                # error every time thru the loop
                config['config_file_mtime'] = os.path.getmtime(config['config_file'])

        sleep(1)


# subs_qr()
#
# substitutes the queue_root for the string %QUEUE_ROOT variable;
# otherwise returns the input string unchanged
#
# Inputs:
# - instr -- the raw string read from the config file
# - queue_root -- the queue_root to be substituted into the input string
#
# Returns:
# - the result of the substitution
def subs_qr(instr, queue_root):
    return re.sub('%QUEUE_ROOT', queue_root, instr)


# pretty_pathname()
#
# For log readability, removes the queue_root from the input pathname
#
# Inputs:
# - config -- the config dict, originally read from the config file
# - path -- the fully-qualified path 
#
# Outputs:
# the shortened string
def pretty_pathname(config, path):
    return re.sub("^%s/" % config['queue_root'], ".../", path)


# lookup_user()
#
# Returns the uid of the named user, or raises a ValueError if that
# user isn't found in the password database
#
# Inputs:
# - username -- the username to look up
# - err_str -- part of the error string to print in the event of a lookup error
#
# Outputs:
# the uid, as an int
def lookup_user(username, err_str):
    try:
        return pwd.getpwnam(username)[2]
    except KeyError:
        raise ValueError("Invalid owner '%s' for %s" % (username, err_str))


# lookup_group()
#
# Returns the gid of the named group, or raises a ValueError if that
# group isn't found in the password database
#
# Inputs:
# - grpname -- the username to look up
# - err_str -- part of the error string to print in the event of a lookup error
#
# Outputs:
# the gid, as an int
def lookup_group(grpname, err_str):
    try:
        return grp.getgrnam(grpname)[2]
    except KeyError:
        raise ValueError("Invalid group '%s' for %s" % (grpname, err_str))


# parse_config_file()
#
# Parses the configuration file and returns it as a config object
#
# Inputs:
# - filename -- the filename to read
#
# Outputs:
# - config -- the parsed contents of the configuration file.  This is
#   a dictionary with keys that loosely correspond to the items in the
#   config file.
#
# The config file structure uses Paython's ConfigParser syntax.  Here
# is a sample config file:
#
#   [LoadBalancer]
#   ; how long GTO files should remain active, in hours. Default=72
#   expiration=72
#   
#   ; how many GeneTorrent instances should be assigned to an upload GTO
#   upload_factor=1
#   
#   ; how many GeneTorrent instances should be assigned to a download GTO
#   download_factor=3
#   
#   ; the interval in seconds after which GTLoadBalancer will refresh its internal
#   ; representation of the state of each of the instance queues. Default = 1800
#   ; (e.g., a half-hour)
#   refresh_interval=1800
#   
#   ; the location to find the gtoinfo utility. Default=/usr/bin/gtoinfo
#   gtoinfo=/Users/howdy/Code/newGeneTorrent/scripts/gtoinfo
#   
#   ; the root of the directory trees; can be referenced below as %QUEUE_ROOT
#   queue_root=/Users/howdy/tmp/Annai/queuedir
#   
#   ; this directory is where new GTOs for upload will be dropped.  GTLoadBalancer
#   ; will pick up the GTOs from this directory and place them in the appropriate
#   ; number of work-queues.
#   upload_queue=%QUEUE_ROOT/dropzone-upload
#   
#   ; this directory is where new GTOs for download will be dropped.  LoadBalancer
#   ; will pick up the GTOs from this directory and place them in the appropriate
#   ; number of work-queues.
#   download_queue=%QUEUE_ROOT/dropzone-download
#   
#   [SaveUploadGTOs]
#   ; If true, GTLoadBalancer will save aside upload GTOs
#   save_upload_gtos=Yes
#   
#   ; Root dir to save upload GTOs.  A GTO named "foo.gto" will be saved
#   ; to {root-dir}/foo/foo.gto
#   save_root_dir=/Users/howdy/tmp/Annai/savedir
#   
#   ; The permission to set the UUID directory to; default=0755
#   save_uuid_dir_perms=0755
#   
#   ; The owner for the UUID directory; default=gtorrent
#   save_uuid_dir_owner=gtorrent
#   
#   ; The group for the UUID directory; default=gtorrent
#   save_uuid_dir_group=gtorrent
#   
#   ; The permissions for the GTO file itself; default=0644
#   save_gto_perms=0644
#   
#   ; The owner for the GTO file itself; default=gtorrent
#   save_gto_owner=gtorrent
#   
#   ; The group for the GTO file itself; default=gtorrent
#   save_gto_group=gtorrent
#   
#   [ValidateDownloadGTOs]
#   ; If true, GTLoadBalancer will validate download GTOs, by ensuring
#   ; that all the files listed in them exist and have the expected file
#   ; lengths.  (This is a reasonably low-cost test of CGHub consistency.)
#   ;
#   ; Note that the variable save_root_dir must be set in the section
#   ; above, because it is used by the validation process.
#   validate_download_gtos=Yes
#   
#   [Instances]
#   ; this section defines the number of work-queues, and the queue directory for each
#   num_instances=6
#   queue_00=%QUEUE_ROOT/dropzone-00
#   queue_01=%QUEUE_ROOT/dropzone-01
#   queue_02=%QUEUE_ROOT/dropzone-02
#   queue_03=%QUEUE_ROOT/dropzone-03
#   queue_04=%QUEUE_ROOT/dropzone-04
#   queue_05=%QUEUE_ROOT/dropzone-05
#   
def parse_config_file(cfg_file):
    defaults = {'gtoinfo' : '/usr/bin/gtoinfo', 
                'expiration' : 72,
                'refresh_interval' : 1800,
                'save_uuid_dir_perms' : '0755',
                'save_uuid_dir_owner' : 'gtorrent',
                'save_uuid_dir_group' : 'gtorrent',
                'save_gto_perms' : '0644',
                'save_gto_owner' : 'gtorrent',
                'save_gto_group' : 'gtorrent',
                }

    cfgparser = ConfigParser.RawConfigParser(defaults)
    cfgparser.readfp(open(cfg_file, "r"))

    config = dict()

    config['config_file'] = cfg_file
    config['config_file_mtime'] = os.path.getmtime(cfg_file)

    # LoadBalancer section
    config['expiration'] = cfgparser.getint('LoadBalancer', 'expiration')
    if (config['expiration'] > 720 or config['expiration'] < 1):
        raise ValueError("Illegal value %d for expiration" % config['expiration'])

    config['upload_factor'] = cfgparser.getint('LoadBalancer', 'upload_factor')
    if (config['upload_factor'] <= 0):
        raise ValueError("Illegal upload_factor %d" % config['upload_factor'])

    config['download_factor'] = cfgparser.getint('LoadBalancer', 'download_factor')
    if (config['download_factor'] <= 0):
        raise ValueError("Illegal download_factor %d" % config['download_factor'])

    config['refresh_interval'] = cfgparser.getint('LoadBalancer', 'refresh_interval')
    if (config['refresh_interval'] <= 10 or config['refresh_interval'] > 86400):
        raise ValueError("Illegal refresh_interval %d" % config['refresh_interval'])

    config['gtoinfo'] = cfgparser.get('LoadBalancer', 'gtoinfo')
    if (not (os.path.isfile(config['gtoinfo']) and os.access(config['gtoinfo'],os.X_OK))):
        raise ValueError("Cannot execute gtoinfo tool at %s" % config['gtoinfo'])

    config['queue_root'] = cfgparser.get('LoadBalancer', 'queue_root')

    config['upload_q'] = subs_qr(cfgparser.get('LoadBalancer', 'upload_queue'), 
                                 config['queue_root'])
    if (not os.path.isdir(config['upload_q'])):
        raise ValueError("Upload queue directory %s doesn't exist" % config['upload_q'])

    config['download_q'] = subs_qr(cfgparser.get('LoadBalancer', 'download_queue'), 
                                   config['queue_root'])
    if (not os.path.isdir(config['download_q'])):
        raise ValueError("Download queue directory %s doesn't exist" % config['download_q'])

    # SaveUploadGTOs section
    config['save_upload_gtos'] = cfgparser.getboolean('SaveUploadGTOs', 'save_upload_gtos')

    if (config['save_upload_gtos']):
        config['save_root_dir'] = \
            subs_qr(cfgparser.get('SaveUploadGTOs', 'save_root_dir'), config['queue_root'])
        if (not os.path.isdir(config['save_root_dir'])):
            raise ValueError("Save root directory %s doesn't exist" % config['save_root_dir'])

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_uuid_dir_perms')
        config['save_uuid_dir_perms'] = int(tmp_str,8)
        if (config['save_uuid_dir_perms'] < 0400 or config['save_uuid_dir_perms'] > 0777):
            raise ValueError("Illegal permissions '%s' for UUID dir" % tmp_str)

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_uuid_dir_owner')
        config['save_uuid_dir_owner'] = lookup_user(tmp_str, "UUID dir")

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_uuid_dir_group')
        config['save_uuid_dir_group'] = lookup_group(tmp_str, "UUID dir")

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_gto_perms')
        config['save_gto_perms'] = int(tmp_str, 8)
        if (config['save_gto_perms'] < 0400 or config['save_gto_perms'] > 0777):
            raise ValueError("Illegal permissions '%s' for GTO permissions" % tmp_str)

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_gto_owner')
        config['save_gto_owner'] = lookup_user(tmp_str, "GTO files")

        tmp_str = cfgparser.get('SaveUploadGTOs', 'save_gto_group')
        config['save_gto_group'] = lookup_group(tmp_str, "GTO files")

    # ValidateDownloadGTOs section
    config['validate_download_gtos'] \
        = cfgparser.getboolean('ValidateDownloadGTOs', 'validate_download_gtos')

    # Instances section
    config['num_instances'] = cfgparser.getint('Instances', 'num_instances')
    if (config['num_instances'] <= 0):
        raise ValueError("Illegal num_instances of %d" % config['num_instances'])

    config['instance_qs'] = list()
    for i in range(config['num_instances']):
        q_dir = subs_qr(cfgparser.get('Instances', 'queue_%02d' % i), config['queue_root'])
        if (not os.path.isdir(q_dir)):
            raise ValueError("Instance %d queue directory %s doesn't exist" % (i, q_dir))
        else:
            config['instance_qs'].append(q_dir)

    # some additional error checking
    if (config['upload_factor'] > config['num_instances']):
        raise ValueError("Upload factor %d is greater than %d configured instances"
                         % (config['upload_factor'], config['num_instances']))

    if (config['download_factor'] > config['num_instances']):
        raise ValueError("Download factor %d is greater than %d configured instances"
                         % (config['download_factor'], config['num_instances']))

    # Log all the parameters we are returning, for debugging
    log(syslog.LOG_DEBUG, "Read configuration file %s successfully:" % cfg_file)
    for k,v in sorted(config.items()):
        log(syslog.LOG_DEBUG, "  %s: %s" % (k, v))

    return config


# main()
#
# Parses the cmd line options and the config file, and then launches
# the mainloop
#
# Return the exit code
def main():
    syslog.openlog(os.path.basename(sys.argv[0]), syslog.LOG_PID, syslog.LOG_LOCAL0)

    # Parse the command line parameters
    parser = OptionParser(description='Perform load balancing between multiple ' \
                              'GeneTorrent server instances.', 
                          version=("%%prog %s" % revlevel),
                          usage="usage: %prog [options]")

    parser.add_option('-c', '--config-file', action='store', 
                      default="/etc/gnos.d/GTLoadBalancer.conf", dest='config_file',
                      help='the location of the configuration file; default is %default')

    (params, other_args) = parser.parse_args()

    if (len(other_args) != 0):
        parser.print_help()
        return 1

    try:
        config = parse_config_file(params.config_file)
    except Exception as e:
        msg = "Fatal error reading config file: %s" % e
        log(syslog.LOG_ALERT, msg)
        print >> sys.stderr, msg
        return -1

    try:
        mainloop(config)
    except (KeyboardInterrupt):
        log(syslog.LOG_NOTICE, "Received keyboard interrupt; terminating")
    except:
        log(syslog.LOG_ALERT, "Uncaught exception: %s" % sys.exc_info()[1])
        log(syslog.LOG_ALERT, "Backtrace: %s" % traceback.extract_tb(sys.exc_info()[2]))
        log(syslog.LOG_ALERT, "Terminating")
        return -1


    return 0


#
# Check the python version for compatability.  Python 3 is not
# backward compatible.
#
if sys.version_info[0] != 2 or sys.version_info[1] < 6:
    print("This script requires Python version 2.6")
    sys.exit(1)

if __name__ == "__main__":
    sys.exit(main())
